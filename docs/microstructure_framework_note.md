I. Introduction & Our Project
***
Arbitrage is typically introduced in its most idealized form, a risk-free profit obtained by purchasing an asset at a lower price in one market and simultaneously selling it at a higher price in another. In textbook settings, the existence of a price discrepancy implies a temporary market efficiency, and the act of arbitrage restores equilibrium. In practice, and in my case within fragmented prediction markets operating across independent platforms (Kalshi and Polymarket for now), this definition proves to be incomplete.
Cross-platform prediction markets present a uniquely fertile environment for structural inefficiencies. Liquidity is segmented, participant bases differ across venues, and contract specifications, while often economically aligned, are not always mechanically identical. Because larger institutional capital has only recently begun engaging these markets, price gaps can persist longer than in traditional asset classes. At first glance, such discrepancies appear to satisfy the textbook definition of arbitrage. However, quoted spreads alone do not determine realizable profit.
Between the observed price difference and the final outcome lies a sequence of constraints: depth limitations, nonlinear slippage, fee friction, latency dispersion, resolution alignment, and capital scaling effects. What appears “risk-free” at the level of top-of-book prices may deteriorate materially once actual execution mechanics are introduced. Arbitrage in these environments is therefore not just a pricing problem, but an execution-constrained microstructure problem. As our framework evolved, an additional dimension showed to be equally important, which is time.
Even when a spread is executable at entry, convergence is not guaranteed to proceed uniformly. Spreads may widen before compressing, stall within narrow bands, or persist until the underlying event resolves. Capital can become trapped in stagnant positions despite the theoretical edge remaining positive. These dynamics require structured exit governance rather than passive convergence assumptions.
My partner and I’s vision emerged from this realization. What began as a search for cross-platform price discrepancies evolved into a research framework centered on the mechanics that determine whether theoretical edge is executable, scalable, and temporally defensible. The system models: 

Depth-aware execution through orderbook walking & VWAP,

Friction-adjusted edge decomposition,

Liquidity-constrained sizing,

Temporal persistence and decay of opportunities,

And governance-based exit logic responsive to spread behavior.

The objective of this paper is not to present performance results. Instead, it formalizes the architectural reasoning behind our project and argues that arbitrage in fragmented prediction markets must be understood as constrained capital allocation under execution friction and temporal uncertainty. In doing so, it reframes arbitrage from a static price discrepancy to a structured process governed by liquidity, friction, and time.


II. Execution Realism
***
In fragmented prediction markets, top-of-book spreads frequently appear wide enough to imply attractive arbitrage. However, these quoted prices typically reflect only marginal liquidity. The depth available at the best bid or best ask is often insufficient to support meaningful trade size.
In limit orderbook markets, execution price is determined not by the best displayed quote, but by the depth profile across multiple levels. When trade size exceeds the liquidity available at the top level, the order must “walk the book,” consuming progressively less favorable price levels. The resulting volume weighted average price (VWAP) may differ materially from the best quoted price. This degradation is nonlinear: small trades may execute near the top of the book, while larger trades experience disproportionately higher slippage as they penetrate deeper levels.
Prediction markets amplify this effect due to typically thinner books and uneven liquidity distribution across venues. A spread that appears economically meaningful at a one-contract level may collapse once scaled to realistic capital allocation. Additionally, partial fill risk becomes nontrivial. If one side of the trade exhausts available liquidity before the other, the position may become directionally exposed rather than market-neutral.
These early observations led to the construction of my execution realism module. Rather than treating quoted spreads as directly executable, the module simulates orderbook walking to compute VWAP adjusted execution prices and explicitly detects partial fill conditions. By modeling slippage as a function of trade size and depth curvature, it reframes arbitrage evaluation as a liquidity aware exercise rather than a static price comparison. This resulted in a shift in perspective: theoretical edge exists only to the extent that it survives depth-aware execution modeling. Execution realism therefore becomes the foundation upon which all subsequent edge evaluation and governance logic must rest.


III. Edge Decomposition, Liquidity Constrained Sizing & Structural Normalization
***
Once execution realism reframes quoted prices as insufficient, the concept of edge itself must be reconsidered. A raw cross-venue spread does not account for execution degradation, fee friction, latency asymmetry, or structural differences in how markets are constructed across platforms. We quickly realized we cannot assume just a frictionless execution, but also structural equivalence between contracts, a condition that is often violated in categorical prediction markets.
The need for structural normalization becomes particularly clear in multi-outcome markets. Consider a categorical market such as “Next UK Prime Minister in 2026.” Kalshi may list 61 possible candidates, while Polymarket lists only 20. Because total probability mass must sum to 100%, the presence of additional long-shot candidates necessarily redistributes probability across the entire outcome set. A candidate priced at 16¢ on a platform with 61 total outcomes is not mathematically equivalent to that same candidate priced at 25¢ on a platform offering only 20 outcomes. The naive price difference may reflect nothing more than differing probability mass allocation across a differently sized outcome space. Such discrepancies create phantom spreads that vanish once structural differences are accounted for.
To prevent execution on these structural illusions, our framework incorporates an adjusted spread calculation designed specifically for categorical markets. The normalization process proceeds in three steps. First, the system identifies the overlapping candidates present on both platforms. Second, it calculates the total probability mass of this shared subset on each venue, isolating what may be considered the comparable outcome space. Third, the candidate’s price is mathematically normalized to this shared probability space before computing the cross-platform spread. Only after this normalization is the spread treated as a candidate for further execution analysis.
This adjustment is strictly required in categorical markets, where outcome sets differ in size and composition. In binary markets the structural symmetry is naturally preserved across venues. In those cases, no probability mass normalization is required, and the raw spread functions as the adjusted spread. The distinction reinforces a broader principle: arbitrage analysis must account for how probability is distributed across the entire market, not just how a single outcome is priced.
With structural normalization in place, edge decomposition proceeds through layered friction adjustments. The adjusted spread is first recalculated using VWAP execution prices rather than top-of-book quotes. Venue-specific fees are then applied, followed by latency-aware considerations. At each step, theoretical edge is reduced toward a realizable net edge.
An additional constraint emerges from dimensional consistency. In prediction markets, contracts are priced on a bounded scale (0 and 1), while capital allocation decisions are denominated in currency. Edge expressed in cents per contract must therefore be translated into return on deployed capital. As trade size increases, slippage and depth exhaustion reduce marginal edge, imposing a natural liquidity ceiling beyond which additional capital degrades return efficiency.
These considerations motivated the construction of my edge decomposition engine. Rather than reporting a single spread metric, the engine produces a layered breakdown: adjusted spread, VWAP-adjusted edge, fee-adjusted edge, latency-adjusted edge, and executable quantity. Opportunities are evaluated not merely on magnitude, but on structural validity and scalability. In doing so, arbitrage is reframed as a constrained capital allocation problem governed simultaneously by probability mass structure, execution friction, and liquidity curvature.


IV. Temporal Dynamics & Exit Governance
***
Execution realism and edge decomposition determine whether a spread is structurally valid and executable at entry. They do not determine whether it will converge.
Cross-venue arbitrage in prediction markets is inherently time-dependent. Even when a spread is entered at positive net edge, its subsequent path may not be monotonic. Spreads may widen before compressing, stall within narrow variance bands, or persist until the underlying real-world event resolves. During this period, capital remains deployed and exposed to both liquidity and temporal risk. As a result, arbitrage must be treated not as a static condition, but as a dynamic state variable evolving over time.
The persistence module models this state evolution by identifying contiguous intervals during which net edge remains above threshold. This enables estimation of opportunity duration and detection of structural stalls. However, persistence measurement alone does not govern capital allocation. A formal exit framework is required.
The governance layer operates through a hierarchy of mathematically defined conditions:
1. Stop-Loss Trigger (Structural Regime Shift Detection)
 To protect against structural breakdowns, the system exits if the spread widens to at least 1.25× the entry spread, provided that top-of-book liquidity remains sufficient and the widened condition persists for two consecutive tracking intervals. The widening threshold is proportional to the original entry condition rather than absolute, ensuring scale invariance. Requiring persistence across intervals reduces false triggers from transient liquidity gaps. Mathematically, this rule guards against adverse drift exceeding 25% of the initial spread magnitude under sufficient liquidity confirmation.
2. Target Capture Rule (Z% Convergence Capture)
 Rather than waiting for full convergence, the system exits when the spread compresses to a predetermined fraction of the initial spread. Let S₀​ denote entry spread and Sₜ​ current spread. Exit occurs when:
Sₜ ≤ (1-Z) S₀​
where Z is the target capture fraction. This converts theoretical convergence into realized profit before resolution and reduces exposure to reversal risk.
3. Floor Detection Exit (Stall Identification)
 Spreads may compress partially and then stagnate within a narrow variance band. If the spread remains within ±0.25¢ of its recent level for at least three consecutive days, and net return after fees remains ≥ 10%, the position is closed to release capital. This rule formalizes the intuition that capital trapped in low-volatility stasis carries opportunity cost even if nominal edge remains positive.
4. Time and Resolution Fallbacks
 If none of the above conditions are met, the position is exited upon reaching a maximum holding horizon or held to event resolution. These rules bound exposure duration and prevent indefinite capital entrapment.
Together, these mechanisms form a temporal risk management layer. The stop-loss rule protects against structural divergence, the capture rule monetizes early convergence, the floor rule mitigates stall risk, and the fallback conditions bind duration.
Thinking about this with Bayesian logic, each rule can be interpreted as a thresholded response to updated posterior beliefs about the probability of convergence conditioned on elapsed time and observed spread behavior. At entry, the trader implicitly assigns a belief that the spread will converge within a reasonable horizon. As time passes without compression, or as the spread widens beyond entry bounds, the posterior belief in favorable convergence rationally declines. The threshold rules serve as structured approximations of this belief updating process, translating evolving microstructure states into capital allocation decisions without requiring full probabilistic calibration.
The key insight is that arbitrage is not simply about detecting price differences, but about managing the joint distribution of convergence magnitude and convergence time. Execution realism determines entry viability; governance determines whether capital remains deployed as the state evolves.


V. Conclusion, Portability & Structural Generalization
***
The development of our project reflects a progression from textbook arbitrage theory to execution-constrained microstructure analysis. What initially began with the hope of finding simple cross-platform price discrepancies in prediction markets revealed deeper structural constraints: depth curvature, friction layering, probability mass mismatches, liquidity ceilings, and temporal uncertainty. Each module within the framework emerged as a response to one of these constraints, transforming arbitrage evaluation from a static price comparison into a structured capital allocation process.
A central implication of this architecture is that its components are not venue-specific. The execution realism logic operates on generic limit orderbook structures. The adjusted spread framework addresses probability mass normalization in categorical markets. The edge decomposition engine formalizes friction layering independent of asset class. The temporal governance layer models convergence risk as a dynamic state variable rather than a binary outcome. These abstractions extend naturally beyond prediction markets.
The crypto microstructure portability demonstration illustrates this generalization. By applying the same VWAP walking and slippage modeling logic to a continuous-price orderbook, the framework confirms that its core abstractions (depth-aware execution, nonlinear slippage scaling, and liquidity-sensitive sizing) are not dependent on bounded contract pricing or binary payoff structures. This portability reinforces the view that arbitrage analysis is fundamentally a microstructure problem rather than a venue-specific strategy.
At its core, the framework reframes arbitrage as execution-constrained capital deployment under fragmentation and time. Apparent spreads must survive structural normalization, depth-aware execution, friction adjustment, and temporal governance before they can be considered economically meaningful. In this sense, cross platform arbitrage is not merely the presence of a price discrepancy, but the disciplined management of liquidity, probability mass, and convergence risk. In this framework, arbitrage is not assumed to be risk-free; it is modeled as risk-managed. 
